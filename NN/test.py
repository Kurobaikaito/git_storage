import torch
import os
import numpy as np
from torch.utils.data import TensorDataset,DataLoader
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
'''
breast_cancer = load_breast_cancer()
X = breast_cancer["data"]
y = breast_cancer["target"]

X_tensor = torch.tensor(X,dtype=torch.float32)
y_tensor = torch.tensor(y,dtype=torch.float32)

X_train, X_test, y_train, y_test = train_test_split(X_tensor,y_tensor,test_size=0.2,random_state=42)

train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

# 创建 DataLoader
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
'''

import pandas as pd
labels = [[1.],[0.],[0.],[1.],[1.],[0.],[0.],[0.],[1.]]
data = [[1.2470e+01, 1.8600e+01, 8.1090e+01, 4.8190e+02, 9.9650e-02, 1.0580e-01,
         8.0050e-02, 3.8210e-02, 1.9250e-01, 6.3730e-02, 3.9610e-01, 1.0440e+00,
         2.4970e+00, 3.0290e+01, 6.9530e-03, 1.9110e-02, 2.7010e-02, 1.0370e-02,
         1.7820e-02, 3.5860e-03, 1.4970e+01, 2.4640e+01, 9.6050e+01, 6.7790e+02,
         1.4260e-01, 2.3780e-01, 2.6710e-01, 1.0150e-01, 3.0140e-01, 8.7500e-02]
,[1.6260e+01, 2.1880e+01, 1.0750e+02, 8.2680e+02, 1.1650e-01, 1.2830e-01,
         1.7990e-01, 7.9810e-02, 1.8690e-01, 6.5320e-02, 5.7060e-01, 1.4570e+00,
         2.9610e+00, 5.7720e+01, 1.0560e-02, 3.7560e-02, 5.8390e-02, 1.1860e-02,
         4.0220e-02, 6.1870e-03, 1.7730e+01, 2.5210e+01, 1.1370e+02, 9.7520e+02,
         1.4260e-01, 2.1160e-01, 3.3440e-01, 1.0470e-01, 2.7360e-01, 7.9530e-02]
,[1.9100e+01, 2.6290e+01, 1.2910e+02, 1.1320e+03, 1.2150e-01, 1.7910e-01,
         1.9370e-01, 1.4690e-01, 1.6340e-01, 7.2240e-02, 5.1900e-01, 2.9100e+00,
         5.8010e+00, 6.7100e+01, 7.5450e-03, 6.0500e-02, 2.1340e-02, 1.8430e-02,
         3.0560e-02, 1.0390e-02, 2.0330e+01, 3.2720e+01, 1.4130e+02, 1.2980e+03,
         1.3920e-01, 2.8170e-01, 2.4320e-01, 1.8410e-01, 2.3110e-01, 9.2030e-02]
,[1.1340e+01, 2.1260e+01, 7.2480e+01, 3.9650e+02, 8.7590e-02, 6.5750e-02,
         5.1330e-02, 1.8990e-02, 1.4870e-01, 6.5290e-02, 2.3440e-01, 9.8610e-01,
         1.5970e+00, 1.6410e+01, 9.1130e-03, 1.5570e-02, 2.4430e-02, 6.4350e-03,
         1.5680e-02, 2.4770e-03, 1.3010e+01, 2.9150e+01, 8.3990e+01, 5.1810e+02,
         1.6990e-01, 2.1960e-01, 3.1200e-01, 8.2780e-02, 2.8290e-01, 8.8320e-02]
,[1.1670e+01, 2.0020e+01, 7.5210e+01, 4.1620e+02, 1.0160e-01, 9.4530e-02,
         4.2000e-02, 2.1570e-02, 1.8590e-01, 6.4610e-02, 2.0670e-01, 8.7450e-01,
         1.3930e+00, 1.5340e+01, 5.2510e-03, 1.7270e-02, 1.8400e-02, 5.2980e-03,
         1.4490e-02, 2.6710e-03, 1.3350e+01, 2.8810e+01, 8.7000e+01, 5.5060e+02,
         1.5500e-01, 2.9640e-01, 2.7580e-01, 8.1200e-02, 3.2060e-01, 8.9500e-02]
,[1.3480e+01, 2.0820e+01, 8.8400e+01, 5.5920e+02, 1.0160e-01, 1.2550e-01,
         1.0630e-01, 5.4390e-02, 1.7200e-01, 6.4190e-02, 2.1300e-01, 5.9140e-01,
         1.5450e+00, 1.8520e+01, 5.3670e-03, 2.2390e-02, 3.0490e-02, 1.2620e-02,
         1.3770e-02, 3.1870e-03, 1.5530e+01, 2.6020e+01, 1.0730e+02, 7.4040e+02,
         1.6100e-01, 4.2250e-01, 5.0300e-01, 2.2580e-01, 2.8070e-01, 1.0710e-01]
,[1.6460e+01, 2.0110e+01, 1.0930e+02, 8.3290e+02, 9.8310e-02, 1.5560e-01,
         1.7930e-01, 8.8660e-02, 1.7940e-01, 6.3230e-02, 3.0370e-01, 1.2840e+00,
         2.4820e+00, 3.1590e+01, 6.6270e-03, 4.0940e-02, 5.3710e-02, 1.8130e-02,
         1.6820e-02, 4.5840e-03, 1.7790e+01, 2.8450e+01, 1.2350e+02, 9.8120e+02,
         1.4150e-01, 4.6670e-01, 5.8620e-01, 2.0350e-01, 3.0540e-01, 9.5190e-02]
,[1.4480e+01, 2.1460e+01, 9.4250e+01, 6.4820e+02, 9.4440e-02, 9.9470e-02,
         1.2040e-01, 4.9380e-02, 2.0750e-01, 5.6360e-02, 4.2040e-01, 2.2200e+00,
         3.3010e+00, 3.8870e+01, 9.3690e-03, 2.9830e-02, 5.3710e-02, 1.7610e-02,
         2.4180e-02, 3.2490e-03, 1.6210e+01, 2.9250e+01, 1.0840e+02, 8.0890e+02,
         1.3060e-01, 1.9760e-01, 3.3490e-01, 1.2250e-01, 3.0200e-01, 6.8460e-02]
,[1.0820e+01, 2.4210e+01, 6.8890e+01, 3.6160e+02, 8.1920e-02, 6.6020e-02,
         1.5480e-02, 8.1600e-03, 1.9760e-01, 6.3280e-02, 5.1960e-01, 1.9180e+00,
         3.5640e+00, 3.3000e+01, 8.2630e-03, 1.8700e-02, 1.2770e-02, 5.9170e-03,
         2.4660e-02, 2.9770e-03, 1.3030e+01, 3.1450e+01, 8.3900e+01, 5.0560e+02,
         1.2040e-01, 1.6330e-01, 6.1940e-02, 3.2640e-02, 3.0590e-01, 7.6260e-02]
]
grads = [[ 1.2075e-03, -3.5275e-03,  6.1960e-04, -7.7800e-04,  6.6718e-04,
         -7.2844e-03,  1.6655e-03, -2.5239e-03, -8.7274e-03,  9.0206e-03,
         -4.2019e-03, -7.2530e-03,  1.6328e-05,  1.1138e-03, -1.7431e-03,
         -5.2046e-03, -4.8536e-03,  5.5336e-03, -1.0208e-02, -1.2267e-02,
          2.2727e-03,  1.7568e-03, -5.9776e-03,  1.1952e-03, -8.9934e-03,
          3.9749e-03,  3.3249e-03,  8.7700e-03, -4.7909e-03, -3.0830e-03]
,[-0.0050,  0.0154, -0.0023,  0.0021, -0.0019,  0.0309, -0.0073,  0.0110,
          0.0365, -0.0371,  0.0179,  0.0298, -0.0005, -0.0045,  0.0067,  0.0220,
          0.0196, -0.0242,  0.0435,  0.0509, -0.0101, -0.0071,  0.0248, -0.0042,
          0.0381, -0.0171, -0.0132, -0.0370,  0.0203,  0.0133]
,[-1.0114e-03,  3.1937e-03, -3.9825e-04,  2.9925e-04, -2.9662e-04,
          6.3770e-03, -1.5548e-03,  2.3503e-03,  7.4807e-03, -7.5876e-03,
          3.7026e-03,  6.1239e-03, -7.0948e-05, -9.6114e-04,  1.2072e-03,
          4.6578e-03,  4.0512e-03, -5.0459e-03,  8.9753e-03,  1.0558e-02,
         -2.1768e-03, -1.4696e-03,  5.1191e-03, -7.4757e-04,  7.8832e-03,
         -3.4202e-03, -2.6768e-03, -7.6950e-03,  4.1828e-03,  2.9521e-03]
,[-0.0080, -0.0079,  0.0001, -0.0217,  0.0088, -0.0020, -0.0054, -0.0019,
         -0.0031,  0.0244,  0.0047, -0.0177, -0.0159, -0.0034, -0.0055,  0.0024,
          0.0021, -0.0081,  0.0067, -0.0064, -0.0086, -0.0010,  0.0013,  0.0174,
          0.0126,  0.0020,  0.0051, -0.0028,  0.0049, -0.0099]
,[ 9.5184e-07, -9.3830e-04,  3.3431e-04, -2.7286e-03,  9.7167e-04,
         -7.9435e-04, -2.5819e-04, -1.5107e-04, -1.7660e-03,  3.8783e-03,
          4.8706e-04, -2.7458e-03, -1.8073e-03, -1.8762e-04, -1.3313e-03,
         -4.2391e-04, -8.0453e-04, -9.1870e-04, -7.8053e-04, -2.5822e-03,
         -6.1850e-04,  7.2510e-04, -1.0358e-03,  2.2133e-03, -2.8746e-04,
          8.4935e-04,  1.2133e-03,  8.3534e-04, -6.3156e-05, -8.3267e-04]
,[-2.5893e-03, -1.9429e-04, -1.0162e-03,  2.7263e-03, -1.4103e-05,
         -5.4344e-04, -4.0720e-04, -7.7389e-04,  2.4165e-03, -2.7847e-03,
         -2.2359e-03,  1.5315e-03,  1.3285e-03,  2.8848e-04,  2.5855e-03,
          6.8838e-04,  1.5735e-03,  2.8025e-03,  1.7081e-03,  3.1286e-03,
          1.5033e-04, -2.7432e-03,  2.4060e-03, -2.1209e-03,  2.3790e-03,
         -1.3463e-03, -1.3824e-03, -8.3677e-04,  2.6447e-04, -1.4360e-03]
,[-1.5543e-03,  4.8252e-03, -7.1446e-04,  6.7681e-04, -6.1462e-04,
          9.7077e-03, -2.2636e-03,  3.4805e-03,  1.1460e-02, -1.1724e-02,
          5.6355e-03,  9.4013e-03, -8.9498e-05, -1.4137e-03,  2.0496e-03,
          6.9239e-03,  6.1960e-03, -7.5603e-03,  1.3610e-02,  1.6010e-02,
         -3.1676e-03, -2.2213e-03,  7.7577e-03, -1.3192e-03,  1.1911e-02,
         -5.3354e-03, -4.1643e-03, -1.1613e-02,  6.3121e-03,  4.2587e-03]
,[-0.0006,  0.0032, -0.0005,  0.0013, -0.0008,  0.0062, -0.0012,  0.0022,
          0.0073, -0.0084,  0.0034,  0.0066,  0.0006, -0.0008,  0.0016,  0.0042,
          0.0039, -0.0044,  0.0082,  0.0103, -0.0016, -0.0013,  0.0048, -0.0015,
          0.0069, -0.0034, -0.0029, -0.0071,  0.0038,  0.0030]
,[-3.0726e-04, -1.7748e-04,  1.7354e-04, -7.2505e-04,  6.6507e-04,
         -1.1151e-03,  2.0100e-04, -7.4508e-05, -1.1045e-03,  1.8968e-03,
         -4.2947e-04, -1.4315e-03, -4.0403e-04,  6.3094e-04, -7.9812e-04,
         -3.6650e-04, -1.0420e-03,  2.5289e-04, -5.8813e-04, -2.2410e-03,
         -6.1092e-05,  7.5967e-05, -8.4249e-04,  5.8076e-04, -7.1254e-04,
          1.9948e-04,  9.5322e-04,  9.0569e-04, -6.8411e-04, -5.1212e-04]
]

store = {"1":[],"2":[],"3":[],"4":[],"5":[],"6":[],"7":[],"8":[],"9":[]}
index = ["1","2","3","4","5","6","7","8","9"]
for _index,_label in zip(index,labels):
    store[_index].append(_label)
for _index,each_grad in zip(index,grads):
    for _data in each_grad:
        store[_index].append(_data)


df = pd.DataFrame(store)

# 保存到Excel文件
file_path = r"E:\DL_project\NN\grad.xlsx"
df.to_excel(file_path, index=False)

print(f"Excel文件已保存到: {file_path}")











